{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import torch\n",
    "\n",
    "from common import load_x264, split_data, evaluate_ii, evaluate_cc\n",
    "\n",
    "# import argparse\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Configuration\n",
    "random_seed = 33154\n",
    "\n",
    "# Enter names of performance columns to consider\n",
    "performances = [\"rel_kbs\"]\n",
    "\n",
    "# Number of nearest neighbours to consider\n",
    "# Make multiples to allow better budget comparison\n",
    "topk_values = (1, 3, 5, 15, 25)\n",
    "topr_values = (1, 3, 5, 15, 25)\n",
    "\n",
    "## Load and prepare data\n",
    "## Load and prepare data\n",
    "data_dir = \"../data\"\n",
    "perf_matrix, input_features, config_features, all_performances = load_x264(\n",
    "    data_dir=data_dir\n",
    ")\n",
    "\n",
    "print(f\"Loaded data x264\")\n",
    "print(f\"perf_matrix:{perf_matrix.shape}\")\n",
    "print(f\"input_features:{input_features.shape}\")\n",
    "print(f\"config_features:{config_features.shape}\")\n",
    "\n",
    "data_split = split_data(perf_matrix, random_state=random_seed)\n",
    "train_inp = data_split[\"train_inp\"]\n",
    "train_cfg = data_split[\"train_cfg\"]\n",
    "test_inp = data_split[\"test_inp\"]\n",
    "test_cfg = data_split[\"test_cfg\"]\n",
    "\n",
    "# This is a look up for performance measurements from inputname + configurationID\n",
    "input_config_map = (\n",
    "    perf_matrix[[\"inputname\", \"configurationID\"] + performances]\n",
    "    .sort_values([\"inputname\", \"configurationID\"])\n",
    "    .set_index([\"inputname\", \"configurationID\"])\n",
    ")\n",
    "all_input_names = pd.Series(\n",
    "    input_config_map.index.get_level_values(\"inputname\").unique()\n",
    ")\n",
    "all_config_ids = pd.Series(\n",
    "    input_config_map.index.get_level_values(\"configurationID\").unique()\n",
    ")\n",
    "\n",
    "regret_map = input_config_map.groupby(\"inputname\").transform(\n",
    "    lambda x: (x - x.min()).abs() / abs(x.min())\n",
    ")\n",
    "average_mape = regret_map.mean(axis=1)\n",
    "\n",
    "rank_map = input_config_map.groupby(\"inputname\").transform(\n",
    "    lambda x: stats.rankdata(x, method=\"min\")\n",
    ")\n",
    "average_ranks = rank_map.mean(axis=1)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "rank_arr = torch.from_numpy(\n",
    "    rank_map.reset_index()  # .loc[(train_inp, train_cfg), :]\n",
    "    .pivot_table(index=\"inputname\", columns=\"configurationID\", values=performances[0])\n",
    "    .values\n",
    ")\n",
    "regret_arr = torch.from_numpy(\n",
    "    regret_map.reset_index()  # .loc[(train_inp, train_cfg), :]\n",
    "    .pivot_table(index=\"inputname\", columns=\"configurationID\", values=performances[0])\n",
    "    .values\n",
    ")\n",
    "\n",
    "input_arr = torch.from_numpy(input_features.values).float()\n",
    "config_arr = torch.from_numpy(config_features.values).float()\n",
    "\n",
    "train_input_mask = input_features.index.isin(train_inp)\n",
    "test_input_mask = input_features.index.isin(test_inp)\n",
    "\n",
    "train_config_mask = config_features.index.isin(train_cfg)\n",
    "test_config_mask = config_features.index.isin(test_cfg)\n",
    "\n",
    "train_input_arr = input_arr[train_input_mask]\n",
    "train_config_arr = config_arr[train_config_mask]\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_cc = []\n",
    "test_cc = []\n",
    "test_ii_rank = []\n",
    "test_ii_ratio = []\n",
    "test_ii_regret = []\n",
    "\n",
    "\n",
    "for topk in topk_values:\n",
    "    train_cc.append(\n",
    "        evaluate_cc(\n",
    "            config_arr[train_config_mask],\n",
    "            rank_arr=rank_arr[:, train_config_mask],\n",
    "            n_neighbors=topk,\n",
    "            n_recs=topr_values,\n",
    "            # config_mask=train_config_mask\n",
    "        ).numpy()\n",
    "    )\n",
    "\n",
    "    test_cc.append(\n",
    "        evaluate_cc(\n",
    "            config_arr,\n",
    "            rank_arr=rank_arr,\n",
    "            n_neighbors=topk,\n",
    "            n_recs=topr_values,\n",
    "            config_mask=test_config_mask,\n",
    "        ).numpy()\n",
    "    )\n",
    "\n",
    "    test_ii = evaluate_ii(\n",
    "        input_arr,\n",
    "        rank_arr=rank_arr,\n",
    "        regret_arr=regret_arr,\n",
    "        n_neighbors=topk,\n",
    "        n_recs=topr_values,\n",
    "        input_mask=train_input_mask,\n",
    "    )\n",
    "    test_ii_rank.append(test_ii[0].numpy())\n",
    "    test_ii_regret.append(test_ii[1].numpy())\n",
    "    test_ii_ratio.append(test_ii[2].numpy())\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(results, topr_values, topk_values):\n",
    "    df = pd.DataFrame(results, columns=topr_values)\n",
    "    df[\"k\"] = topk_values\n",
    "    df.set_index(\"k\", inplace=True)\n",
    "    df.columns = pd.MultiIndex.from_product([[\"r\"], df.columns])\n",
    "    return df\n",
    "\n",
    "# TODO Scale ii_ranks results\n",
    "# TODO Share results in README\n",
    "\n",
    "print(\"train cc\\n\", prepare_df(train_cc, topr_values, topk_values), \"\\n\")\n",
    "print(\"test cc\\n\", prepare_df(test_cc, topr_values, topk_values), \"\\n\")\n",
    "print(\"ii rank\\n\", prepare_df(test_ii_rank, topr_values, topk_values), \"\\n\")\n",
    "print(\"ii ratio\\n\", prepare_df(test_ii_ratio, topr_values, topk_values), \"\\n\")\n",
    "print(\"ii regret\\n\", prepare_df(test_ii_regret, topr_values, topk_values), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}